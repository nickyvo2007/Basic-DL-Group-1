{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b4fee3",
   "metadata": {},
   "source": [
    "Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f006fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bf4c4aa",
   "metadata": {},
   "source": [
    "Calculus - Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936288af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc1857cb",
   "metadata": {},
   "source": [
    "Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63158209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "134e917e",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f06e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===== Test MSE =====\n",
    "y_true = torch.tensor([10.0,5.0,9.0])\n",
    "y_pred = torch.tensor([8.0,7.0,6.0])\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "print(\"MSE Loss:\", mse(y_pred, y_true).item())\n",
    "\n",
    "# ===== Test Cross Entropy =====\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "outputs = torch.tensor([[2.0, 0.5]])  # logits\n",
    "labels = torch.tensor([0])           # true class index\n",
    "\n",
    "print(\"Cross Entropy Loss:\", ce(outputs, labels).item())   #transform logic to probality, calculate CE, .item() to change to number to print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3ce24",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b1b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74de508d",
   "metadata": {},
   "source": [
    "Autograd and Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4201ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autograd\n",
    "import torch\n",
    "\n",
    "# 1. Setup Data\n",
    "x_train = torch.tensor([1.0, 2.0, 3.0])\n",
    "y_true = torch.tensor([3.0, 5.0, 7.0]) \n",
    "\n",
    "# 2. Initialize Parameters (Starting Point)\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "learning_rate = 0.01\n",
    "\n",
    "# 3. The Loop (Iterative Learning)\n",
    "for epoch in range(100): # Running for 100 iterations\n",
    "    # --- Step A: Forward Pass ---\n",
    "    y_pred = w * x_train + b\n",
    "\n",
    "    \n",
    "    # --- Step B: Calculate Scalar Loss ---\n",
    "    loss = torch.mean((y_pred - y_true)**2)\n",
    "    \n",
    "    # --- Step C: Backward Pass (Autograd) ---\n",
    "    # This computes gradients for w and b\n",
    "    loss.backward()\n",
    "    \n",
    "    # --- Step D: Optimization (Weight Update) ---\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        \n",
    "        # IMPORTANT: Zero the gradients for the next iteration\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, w = {w.item():.2f}, b = {b.item():.2f}\")\n",
    "\n",
    "#Model Architecture\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=128, output_dim=10):\n",
    "        \"\"\"\n",
    "        Constructor: Defines the model architecture.\n",
    "        \"\"\"\n",
    "        super().__init__() # Initializes the base nn.Module\n",
    "        \n",
    "        # Define Fully Connected (Linear) Layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Regularization layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward Pass: Defines the computational graph.\n",
    "        \"\"\"\n",
    "        # Step 1: Reshape 2D image (28x28) to 1D vector (784)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        \n",
    "        # Step 2: Linear transformation followed by Non-linear activation (ReLU)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Step 3: Apply Dropout to prevent overfitting\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 4: Final linear layer to produce output Logits\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "# Customizing the architecture: input 784, hidden 256, output 10\n",
    "model = MNISTClassifier(input_dim=784, hidden_dim=256, output_dim=10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
